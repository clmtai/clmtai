# Computer scientist with limited background in AI / ML

You are a computer scientist interested in contributing to ClmtAI but you have
limited experience and / or background in AI / ML? Then this page with resources
is for you!

The purpose of this page is to evolve into an onboarding guide to help you get up to 
speed as soon as possible and contribute to ClmtAI as we appreciate your
interest and enthousiasm!

We cover introductions by major field of expertise

## Machine Learning Intro

- For an incredible intro to machine learning, you can head to [Machine
  Learning
  Specialization](https://www.coursera.org/specializations/machine-learning-introduction)


## GeoSpatial Intro
- TODO

## LLM Intro
High-level overview and hands-on:
- A mellow introduction to language models and generative AI: [How GPT models work](https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5)
- LLMs from a practical perspective: [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html)
- Putting the golden layer on a language model and make for a human like chat experience: [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)

Deep-dive into language and transformers (the underlying architecture of LLMs):
- Video series giving a nice starting point to understand language and transformers: Visual Guide to Transformers 
  - Part 0: [The Neuroscience of “Attention”](https://www.youtube.com/watch?v=48gBPL7aHJY)
  - Part 1: [Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw)
  - Part 2: [Multi-Head & Self-Attention](https://www.youtube.com/watch?v=mMa2PmYJlCo)
  - Part 3: [Decoder’s Masked Attention](https://www.youtube.com/watch?v=gJ9kaJsE78k&t=172s)
- Technical deep-dive into the underlying transformer architecture: [Transformers from Scratch](https://e2eml.school/transformers.html)
- The OG paper: [Attention is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

More on generative AI in general:
- OpenAI giving an intro to GenAI back in 2016: [Generative Models](https://openai.com/research/generative-models)
- The technique powering good generative output: [Generative Adversarial Nets (GANs)](https://arxiv.org/pdf/1406.2661.pdf)
- That same technique explained in an understandable way: [A Gentle Introduction to Generative Adversarial Networks (GANs)](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)