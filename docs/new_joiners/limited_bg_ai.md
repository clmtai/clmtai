# Computer scientist with limited background in AI / ML

You are a computer scientist interested in contributing to ClmtAI but you have
limited experience and / or background in AI / ML? Then this page with resources
is for you!

The purpose of this page is to evolve into an onboarding guide to help you get up to 
speed as soon as possible and contribute to ClmtAI as we appreciate your
interest and enthousiasm!

We cover introductions by major field of expertise

## Machine Learning Intro

- For an incredible intro to machine learning, you can head to [Machine
  Learning
  Specialization](https://www.coursera.org/specializations/machine-learning-introduction)


## GeoSpatial Intro
- TODO

## LLM Intro
The following resources will give you a thourough introduction to large language models (LLMs) and how to use them. You can either pick one of the resources listed in [All-in-one resources](#0-all-in-one-resources) or follow the chapters 1 to 6 in order. The essential content for each chapter is marked with a **bold** font, the additional resources offer a more in depth view on the topic.
### 0. All-in-one resources
- The ultimate [LLM Learning Mind Map](https://lucid.app/lucidspark/98705f5a-a385-4820-a648-be35c9d1cda6/edit?page=0_0#)
- Intro to LLMs and building applications with LangChain and VectorDBs: [LangChain & Vector Databases in Production](https://learn.activeloop.ai/courses/langchain) - 40 hours course

### 1. High-level overview and hands-on
- **A mellow introduction to language models and generative AI: [How GPT models work](https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5)**
- LLMs from a practical perspective: [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html)
- Putting the golden layer on a language model and make for a human like chat experience: [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)

### 2. Deep-dive into language and transformers (the underlying architecture of LLMs)
- **Video series giving a nice starting point to understand language and transformers: Visual Guide to Transformers**
  - Part 0: [The Neuroscience of “Attention”](https://www.youtube.com/watch?v=48gBPL7aHJY)
  - Part 1: [Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw)
  - Part 2: [Multi-Head & Self-Attention](https://www.youtube.com/watch?v=mMa2PmYJlCo)
  - Part 3: [Decoder’s Masked Attention](https://www.youtube.com/watch?v=gJ9kaJsE78k&t=172s)
- Illustrated guides to Attention and Transformers:
  - Blog post on [Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
  - Blog post on [Transformers](https://jalammar.github.io/illustrated-transformer/)
  - Video on [Transformers](https://www.youtube.com/watch?v=-QH8fRhqFHM)
- Technical deep-dive into the underlying transformer architecture: [Transformers from Scratch](https://e2eml.school/transformers.html)
- The OG paper: [Attention is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

### 3. Fine-tuning LLMs
- **When to apply fine-tuning, how to prepate the data and how to execute it: [Fine-tuning LLMs, Deeplerning.AI](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)** - 1 hour course
- Example of fine-tuning a LLaMa 2 model, including code, data, and hardware requirements: [Fine-tuning LLaMa 2 70B](https://huggingface.co/blog/ram-efficient-pytorch-fsdp)

### 4. Using LLMs
- **Introduction to [Prompt Engineering, Deeplerning.AI](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)** - 1 hour course

### 5. Building applications with LLMs
- **Learn how to chain prompts effectively: [Building Systems with the ChatGPT API, Deeplerning.AI](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)** - 1 hour course
- **Learn the basics of LangChain and how to use them: [LangChain for LLM Application Development, Deeplerning.AI](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)** - 1 hour course


### 6. More on generative AI in general
- OpenAI giving an intro to GenAI back in 2016: [Generative Models](https://openai.com/research/generative-models)
- The technique powering good generative output: [Generative Adversarial Nets (GANs)](https://arxiv.org/pdf/1406.2661.pdf)
- That same technique explained in an understandable way: [A Gentle Introduction to Generative Adversarial Networks (GANs)](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)